import logging
from typing import Any, Dict, List, Optional, Type, Union

from pydantic import BaseModel

from recipe_executor.llm_utils.llm import LLM
from recipe_executor.llm_utils.mcp import get_mcp_server
from recipe_executor.models import FileSpec
from recipe_executor.protocols import ContextProtocol
from recipe_executor.steps.base import BaseStep, StepConfig
from recipe_executor.utils.models import json_object_to_pydantic_model
from recipe_executor.utils.templates import render_template


class LLMGenerateConfig(StepConfig):
    """
    Config for LLMGenerateStep.

    Fields:
        prompt: The prompt to send to the LLM (templated beforehand).
        model: The model identifier to use (provider/model_name format).
        mcp_servers: List of MCP servers for access to tools.
        output_format: The format of the LLM output (text, files, or JSON).
            - text: Plain text output.
            - files: List of files generated by the LLM.
            - object: Object based on the provided JSON schema.
            - list: List of items based on the provided JSON schema.
        output_key: The name under which to store the LLM output in context.
    """

    prompt: str
    model: str = "openai/gpt-4o"
    mcp_servers: Optional[List[Dict[str, Any]]] = None
    output_format: Union[str, Dict[str, Any], List[Any]]
    output_key: str = "llm_output"


class FileSpecCollection(BaseModel):
    files: List[FileSpec]


class LLMGenerateStep(BaseStep[LLMGenerateConfig]):
    def __init__(self, logger: logging.Logger, config: Dict[str, Any]) -> None:
        super().__init__(logger, LLMGenerateConfig(**config))

    async def execute(self, context: ContextProtocol) -> None:
        prompt_str: str = render_template(self.config.prompt, context)
        model_str: str = render_template(self.config.model, context) if self.config.model else "openai/gpt-4o"
        output_key: str = render_template(self.config.output_key, context)
        mcp_server_configs: List[Dict[str, Any]] = []
        mcp_servers: Optional[List[Any]] = None

        # Combine mcp_servers from step config and context config
        step_mcp_servers = self.config.mcp_servers if self.config.mcp_servers is not None else []
        context_mcp_servers = context.get_config().get("mcp_servers", [])
        mcp_server_configs = list(step_mcp_servers) + list(context_mcp_servers)

        if mcp_server_configs:
            mcp_servers = [get_mcp_server(logger=self.logger, config=server_cfg) for server_cfg in mcp_server_configs]
        else:
            mcp_servers = None

        output_format = self.config.output_format
        result: Any = None

        try:
            if output_format == "text":
                llm = LLM(self.logger, model=model_str, mcp_servers=mcp_servers)
                self.logger.debug(f"Calling LLM with model: {model_str} (output: text)")
                result = await llm.generate(prompt_str, output_type=str)
                context[output_key] = result
                return
            elif output_format == "files":
                llm = LLM(self.logger, model=model_str, mcp_servers=mcp_servers)
                self.logger.debug(f"Calling LLM with model: {model_str} (output: files)")
                result = await llm.generate(prompt_str, output_type=FileSpecCollection)
                if isinstance(result, FileSpecCollection):
                    context[output_key] = result.files
                else:
                    # Defensive in case the LLM doesn't return the expected model
                    raise RuntimeError(f"LLM did not return a FileSpecCollection: got {type(result)}")
                return
            elif isinstance(output_format, list):
                # Interpret as list of items; wrap as object with `items` root
                object_schema: Dict[str, Any] = {
                    "type": "object",
                    "properties": {"items": {"type": "array", "items": output_format[0] if output_format else {}}},
                }
                PydanticModel: Type[BaseModel] = json_object_to_pydantic_model(object_schema, model_name="ListModel")
                llm = LLM(self.logger, model=model_str, mcp_servers=mcp_servers)
                self.logger.debug(f"Calling LLM with model: {model_str} (output: list)")
                model_result = await llm.generate(prompt_str, output_type=PydanticModel)
                items = getattr(model_result, "items", None)
                if items is None and isinstance(model_result, dict):
                    items = model_result.get("items")
                if items is None:
                    raise RuntimeError("LLM did not return an object with 'items' for list output format.")
                context[output_key] = items
                return
            elif isinstance(output_format, dict):
                # Interpret as JSON schema for object format
                PydanticModel: Type[BaseModel] = json_object_to_pydantic_model(output_format, model_name="ObjectModel")
                llm = LLM(self.logger, model=model_str, mcp_servers=mcp_servers)
                self.logger.debug(f"Calling LLM with model: {model_str} (output: object)")
                model_result = await llm.generate(prompt_str, output_type=PydanticModel)
                if isinstance(model_result, BaseModel):
                    context[output_key] = model_result.model_dump()
                elif isinstance(model_result, dict):
                    context[output_key] = model_result
                else:
                    raise RuntimeError("LLM returned unexpected type for object output format.")
                return
            else:
                raise ValueError(f"Unsupported output_format: {output_format}")
        except Exception as e:
            self.logger.error(f"LLM call failed for model '{model_str}' with prompt: {prompt_str}\nError: {e}")
            raise
